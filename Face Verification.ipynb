{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Import all the neccessary libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3743280\n"
     ]
    }
   ],
   "source": [
    "#Load the weights already trained, here we load FaceNet model\n",
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n",
    "print(\"Total Params:\", FRmodel.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This model ensures that the output is an encoding vector of size 128. Here we need encoding that can perform two simple task i.e. \n",
    "#  encoding of two images of same person must be close while the encoding of images of two differnt people must vary a lot.\n",
    "# To serve this purpose we define triplet loss function.\n",
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "        \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2] \n",
    "    #  Compute the (encoding) distance between the anchor and the positive \n",
    "    #  Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    #  Subtract the two previous distances and add alpha.\n",
    "    # Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor-positive),axis=-1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor-negative),axis=-1)\n",
    "    basic_loss = pos_dist-neg_dist+alpha\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss,0.0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 528.1432\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as test:\n",
    "    tf.set_random_seed(1)\n",
    "    y_true = (None, None, None)\n",
    "    y_pred = (tf.random_normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=1, stddev=1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=3, stddev=4, seed = 1))\n",
    "    loss = triplet_loss(y_true, y_pred)\n",
    "    \n",
    "    print(\"Loss = \" + str(loss.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FaceNet is trained by minimizing triplet loss, but here we laod weights from a previously trained model. This may take time.\n",
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the model on the our dataset for this we store the encodigs in a dataset, to generate encodings we use the function \n",
    "#which generates the encoding by running forward propagation of the model on the specified image\n",
    "database={}\n",
    "database[\"tanmay\"]=img_to_encoding(\"images/tanmay.jpg\",FRmodel)\n",
    "# database[\"utsav\"]=img_to_encoding(\"images/utsav.jpeg\",FRmodel)\n",
    "database[\"himanshu\"]=img_to_encoding(\"images/himanshu.png\",FRmodel)\n",
    "database[\"mummy\"]=img_to_encoding(\"images/mummy.jpg\",FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the verify function which computes the encoding from the image path provided and then computes the distance between the \n",
    "# encoding and the identity image stored in database and if this distance is less than 0.7 then the person is the same else no.\n",
    "def verify(image_path, identity, database, model):    \n",
    "    #  Compute the encoding for the image. \n",
    "    #  Compute distance with identity's image. \n",
    "    encoding = img_to_encoding(image_path,model) \n",
    "    dist = np.linalg.norm(encoding-database[identity])\n",
    "    \n",
    "    if dist<0.75:\n",
    "        print(\"It's \" + str(identity) + \", Adios!\")\n",
    "        ans = True\n",
    "    else:\n",
    "        print(\"It's not \" + str(identity) + \", sorry\")\n",
    "        ans = False\n",
    "    return dist, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's tanmay, Adios!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.73613393, True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking on our data\n",
    "verify(\"images/verif.jpg\", \"tanmay\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not mummy, sorry\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.93025035, False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/verif.jpg\", \"mummy\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face recognition\n",
    "# The earlier function was to verify whether the name and image match toeach other on the other hand this function will be used to \n",
    "# take path to image as input and then find which person matches to the given image and if no image matches it it returns that\n",
    "# the image is not in the database\n",
    "\n",
    "def who_is_it(image_path, database, model):\n",
    "    ##  Compute the target \"encoding\" for the image. \n",
    "    ##  Find the closest encoding ##\n",
    "    # Initialize \"min_dist\" to a large value, say 100 \n",
    "    encoding = img_to_encoding(image_path,model)\n",
    "    min_dist = 100\n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. \n",
    "        dist = np.linalg.norm(encoding-db_enc)\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name.\n",
    "        if dist<min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "    if min_dist > 0.75:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's tanmay, the distance is 0.73613393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.73613393, 'tanmay')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#implementing the new face recognitin function\n",
    "who_is_it(\"images/verif.jpg\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So here I have implemented a face recognition system using the concept of transfer learning where we use the pretrained weights \n",
    "# in the FaceNet model and then check whether this works upon our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
